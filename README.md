# RAG Data Pipeline ğŸš€

Há»‡ thá»‘ng pipeline tá»± Ä‘á»™ng **lá»c â†’ Ä‘Ã¡nh giÃ¡ â†’ cáº£i thiá»‡n** document, xuáº¥t ra Ä‘á»‹nh dáº¡ng sáºµn sÃ ng cho RAG system. DÃ¹ng **Ollama local** lÃ m AI engine â€” hoÃ n toÃ n offline, khÃ´ng cáº§n API key.

---

## âœ¨ TÃ­nh nÄƒng

- ğŸ” **Lá»c thÃ´ng minh** â€” Loáº¡i bá» tÃ i liá»‡u kÃ©m cháº¥t lÆ°á»£ng, trÃ¹ng láº·p, láº¡c Ä‘á» trÆ°á»›c khi gá»i AI
- ğŸ¤– **AI tá»± Ä‘Ã¡nh giÃ¡** â€” Ollama cháº¥m Ä‘iá»ƒm 5 tiÃªu chÃ­ cháº¥t lÆ°á»£ng theo thang 0â€“1
- ğŸ“ **AI tá»± cáº£i thiá»‡n** â€” Tá»± Ä‘á»™ng rewrite + re-evaluate vÃ²ng láº·p tá»‘i Ä‘a 2 láº§n
- ğŸ·ï¸ **LÃ m giÃ u metadata** â€” AI tá»± táº¡o keywords, summary, topic tags
- âœ‚ï¸ **Smart chunking** â€” TÃ¡ch chunk theo ranh giá»›i cÃ¢u, khÃ´ng cáº¯t giá»¯a Ã½
- ğŸ“¦ **Export chuáº©n RAG** â€” TÆ°Æ¡ng thÃ­ch LangChain `Document` vÃ  LlamaIndex `TextNode`

---

## ğŸ—ï¸ Kiáº¿n trÃºc

```
Input (.txt / .md / .pdf)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 1 â€” FILTERS (rule-based)     â”‚
â”‚  quality â†’ dedup â†’ relevance         â”‚â”€â”€â–º REJECT (ngáº¯n/trÃ¹ng/láº¡c Ä‘á»)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ passed
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 2 â€” EVALUATORS (AI)          â”‚
â”‚  quality + completeness + rag_fit    â”‚â”€â”€â–º REJECT náº¿u score < 0.40
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ IMPROVE (0.40â€“0.75)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 3 â€” IMPROVERS (AI)           â”‚
â”‚  clean â†’ rewrite â†’ re-eval (max 2Ã—)  â”‚â”€â”€â–º REJECT náº¿u váº«n tháº¥p
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ PASS (â‰¥ 0.75)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 4 â€” OUTPUT                   â”‚
â”‚  chunk + metadata â†’ export JSONL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
rag_pipeline/
â”œâ”€â”€ config.py                 # Cáº¥u hÃ¬nh LLM, thresholds, chunking
â”œâ”€â”€ models.py                 # Data schemas dÃ¹ng chung
â”œâ”€â”€ pipeline.py               # Orchestrator end-to-end
â”œâ”€â”€ main.py                   # CLI entry point
â”‚
â”œâ”€â”€ llm/                      # Ollama local layer
â”‚   â”œâ”€â”€ base_llm.py           # Abstract + retry/timeout
â”‚   â”œâ”€â”€ ollama_llm.py         # HTTP â†’ Ollama /api/generate
â”‚   â””â”€â”€ __init__.py           # create_llm(config) factory
â”‚
â”œâ”€â”€ filters/                  # Module 1: Pre-filter (rule-based)
â”‚   â”œâ”€â”€ base_filter.py
â”‚   â”œâ”€â”€ quality_filter.py     # Äá»™ dÃ i + noise ratio
â”‚   â”œâ”€â”€ dedup_filter.py       # MD5 exact + Jaccard near-dup
â”‚   â”œâ”€â”€ relevance_filter.py   # Keyword/topic matching
â”‚   â””â”€â”€ filter_pipeline.py
â”‚
â”œâ”€â”€ evaluators/               # Module 2: AI Self-Evaluation
â”‚   â”œâ”€â”€ base_evaluator.py
â”‚   â”œâ”€â”€ quality_evaluator.py  # coherence, language_quality
â”‚   â”œâ”€â”€ completeness_evaluator.py  # completeness, factual_clarity
â”‚   â”œâ”€â”€ rag_evaluator.py      # rag_suitability, chunk density
â”‚   â””â”€â”€ score_aggregator.py   # Weighted sum â†’ PASS/IMPROVE/REJECT
â”‚
â”œâ”€â”€ improvers/                # Module 3: AI Improvement
â”‚   â”œâ”€â”€ base_improver.py
â”‚   â”œâ”€â”€ text_cleaner.py       # Rule-based: xÃ³a HTML, chuáº©n hoÃ¡
â”‚   â”œâ”€â”€ chunker.py            # Sentence-aware chunking + overlap
â”‚   â”œâ”€â”€ metadata_enricher.py  # AI: keywords, summary, tags
â”‚   â””â”€â”€ improve_pipeline.py   # rewrite â†’ re-eval loop
â”‚
â”œâ”€â”€ output/                   # Module 4: Export
â”‚   â”œâ”€â”€ formatter.py          # LangChain / LlamaIndex schema
â”‚   â””â”€â”€ exporter.py           # Ghi JSONL / JSON / Markdown
â”‚
â””â”€â”€ demo/
    â”œâ”€â”€ sample_data/
    â”‚   â”œâ”€â”€ good_doc.txt
    â”‚   â”œâ”€â”€ noisy_doc.txt
    â”‚   â””â”€â”€ duplicate_doc.txt
    â””â”€â”€ run_demo.py
```

---

## âš™ï¸ CÃ i Ä‘áº·t

### 1. CÃ i Ollama

```bash
# Táº£i táº¡i: https://ollama.com/download
ollama pull llama3.2
```

> CÃ³ thá»ƒ dÃ¹ng model khÃ¡c: `mistral`, `phi3`, `qwen2.5`, `deepseek-r1`

### 2. Python dependencies

```bash
pip install pyyaml
# KhÃ´ng cáº§n gÃ¬ thÃªm â€” urllib cÃ³ sáºµn trong stdlib Python
```

---

## ğŸš€ Sá»­ dá»¥ng

### Cháº¡y demo nhanh

```bash
cd rag_pipeline
python demo/run_demo.py
```

Káº¿t quáº£ mong Ä‘á»£i:

| File | Káº¿t quáº£ |
|---|---|
| `good_doc.txt` | âœ… PASS â†’ 3 chunks exported |
| `noisy_doc.txt` | ğŸ”„ IMPROVE â†’ AI rewrite â†’ âœ… PASS â†’ 2 chunks exported |
| `duplicate_doc.txt` | âŒ REJECT (dedup filter) |

### CLI

```bash
python main.py --input ./my_docs/ --output ./output/
```

### Output files

```
output/
â”œâ”€â”€ documents.jsonl    # RAG-ready chunks (1 dÃ²ng = 1 chunk)
â”œâ”€â”€ eval_report.json   # Äiá»ƒm Ä‘Ã¡nh giÃ¡ tá»«ng document
â””â”€â”€ rejected.json      # Document bá»‹ loáº¡i + lÃ½ do
```

---

## ğŸ“Š Schema Output (LangChain)

Má»—i dÃ²ng trong `documents.jsonl`:

```json
{
  "page_content": "Ná»™i dung chunk...",
  "metadata": {
    "doc_id": "b3f1a...",
    "chunk_id": 0,
    "source": "my_file.txt",
    "keywords": ["RAG", "vector search", "embedding"],
    "summary": "TÃ³m táº¯t ngáº¯n gá»n ná»™i dung chunk.",
    "topic_tags": ["AI", "NLP"],
    "language": "vi",
    "eval_score": 0.82,
    "created_at": "2026-02-24T04:30:00Z"
  }
}
```

### Load vÃ o LangChain

```python
import json
from langchain.schema import Document

with open("output/documents.jsonl") as f:
    docs = [Document(**json.loads(line)) for line in f]
```

---

## ğŸ›ï¸ Cáº¥u hÃ¬nh ([config.py](file:///d:/New%20folder/rag_pipeline/config.py))

```python
CONFIG = {
    "llm": {
        "model": "llama3.2",                    # Ä‘á»•i model táº¡i Ä‘Ã¢y
        "base_url": "http://localhost:11434",
        "temperature": 0.3,
    },
    "evaluation": {
        "pass_threshold": 0.75,                  # PASS náº¿u score â‰¥ 0.75
        "improve_threshold": 0.40,               # IMPROVE náº¿u score â‰¥ 0.40
        "max_improve_attempts": 2,
    },
    "chunking": {
        "chunk_size": 512,                       # tokens
        "chunk_overlap": 64,
    },
}
```

---

## ğŸ¤– Scoring Rubric (AI Evaluator)

| TiÃªu chÃ­ | Trá»ng sá»‘ | MÃ´ táº£ |
|---|---|---|
| `coherence` | 25% | Máº¡ch láº¡c, cÃ¢u liÃªn káº¿t tá»‘t |
| `completeness` | 25% | ThÃ´ng tin Ä‘áº§y Ä‘á»§, khÃ´ng bá»‹ cáº¯t dá»Ÿ |
| `factual_clarity` | 20% | RÃµ rÃ ng, khÃ´ng mÆ¡ há»“ |
| `rag_suitability` | 20% | PhÃ¹ há»£p Ä‘á»ƒ chunk & retrieve |
| `language_quality` | 10% | ChÃ­nh táº£, ngá»¯ phÃ¡p |

AI tráº£ vá» JSON cÃ³ cáº¥u trÃºc:

```json
{
  "scores": { "coherence": 0.85, "completeness": 0.70, ... },
  "reasoning": "Document máº¡ch láº¡c nhÆ°ng thiáº¿u káº¿t luáº­n.",
  "improvement_hints": ["Bá»• sung Ä‘oáº¡n káº¿t luáº­n", "LÃ m rÃµ thuáº­t ngá»¯ á»Ÿ Ä‘oáº¡n 2"]
}
```

---

## ğŸ”„ Self-feedback Loop

```
text_cleaner
    â†’ AI rewrite (dÃ¹ng improvement_hints tá»« evaluator)
    â†’ re-evaluate
    â†’ score â‰¥ 0.75 ? PASS
    : attempts < 2  ? loop láº¡i
    : REJECT + ghi vÃ o rejected.json
```

---

## ğŸ“‹ Requirements

- Python 3.10+
- Ollama Ä‘ang cháº¡y (`ollama serve`)
- `pyyaml` (optional, chá»‰ cáº§n náº¿u dÃ¹ng config file YAML)
